---
title: "regression"
author: "Anandu R"
date: "9/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height=5, fig.width=5, fig.align = "centred")
suppressMessages(
  {
    if(!require(manipulate|!require(GGally)|!require(lmtest))){
      install.packages("manipulate")
      install.packages("GGally")
      install.packages("lmtest")
      library(GGally)
      library(manipulate)
      library(lmtest)
    }
    library(UsingR)
    library(dplyr)
    data("galton")
    data("diamond")
    data("trees")
    data("swiss")
  }
)
```

### Loading the galton data
```{r}
library(UsingR)
data("galton")
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData = mutate(freqData, child = as.numeric(as.vector(child)), parent = as.numeric(as.vector(parent)))
```

The data contains the heights of parents and child (paired). We try and estimate 
the child's height(y) using the height of the parents(x).

### Performing regression analysis without the y-intercept
With y-intercept 0 or in other words without a y-intercept, but this doesn't 
result in the best line to  fit the data, 
```{r, echo = T}
Beta = as.numeric(coef(lm(galton$child ~ galton$parent - 1)))
angle = round(atan(Beta)*180/3.14,2)
mse = mean( (galton$child - Beta * galton$parent)^2 )
```  
Using the lm function we find the slope of line to be `r round(Beta, 3)`, and that the 
line that best fits the data originating from the slope is angled at `r angle` 
degrees. Using this value of Beta we can manually find the regression line that
fits the data and the mean square error is found to be `r round(mse, 3)`
```{r}
ggplot(
  galton,
  aes(
    x = parent,
    y = child
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.1) +
  labs(x = "parent", y = "child") +
  geom_abline(slope = Beta, lwd = 4, col = "hotpink2") + 
  geom_point(aes(x = 0, y = 0), cex = 2, pch = 21) +
  geom_smooth(method = "lm", formula = y~x-1, se = F, col = "skyblue", lwd = 2, lty = 2) + 
  ggtitle(paste("beta = ", round(Beta,3), "mse = ", round(mse, 3))) +
  theme_bw() + 
  coord_cartesian(xlim = range(galton$parent), ylim = range(galton$child))
```

We see that the broken blue lines representing the regression line computed
using the lm() function overlaps with the pink line drawn using the Beta
coefficient estimated earlier.

### Regression with centered data
If we were to centre the data around the mean and perform the same analysis, 
we'd get the same result as we'd if the intercept estimator <img src="https://render.githubusercontent.com/render/math?math=$\beta_{0}$">  was under consideration 

```{r, echo = T}
galtonCentered = mutate(galton, child = child - mean(child), parent = parent - mean(parent))
Beta = as.numeric(coef(lm(galtonCentered$child ~ galtonCentered$parent - 1)))
angle = round(atan(Beta)*180/3.14,2)
mse = mean( (galtonCentered$child - Beta * galtonCentered$parent)^2 )
```  
Using the lm function we find the slope of line to be `r round(Beta, 3)`, and that the 
line that best fits the data originating from the slope is angled at `r angle` 
degrees. Using this value of Beta we can manually find the regression line that
fits the data and the mean square error is found to be `r round(mse, 3)`

```{r}
ggplot(
  galtonCentered,
  aes(
    x = parent,
    y = child
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.1) +
  labs(x = "parent", y = "child") +
  geom_abline(slope = Beta, lwd = 4, col = "hotpink2") + 
  geom_point(aes(x = 0, y = 0), cex = 2, pch = 21) +
  geom_smooth(method = "lm", formula = y~x-1, se = F, col = "skyblue", lwd = 2, lty = 2) + 
  ggtitle(paste("beta = ", round(Beta,3), "mse = ", round(mse, 3))) +
  theme_bw() + 
  coord_cartesian(xlim = range(galtonCentered$parent), ylim = range(galtonCentered$child))
```   

We see that the broken blue lines representing the regression line computed
using the lm() function overlaps with the pink line drawn using the Beta
coefficient estimated earlier. 

### Regression with normalized data
If we were to normalize the data and perform the same analysis, we'd get the
same result still, furthmore, the <img src="https://render.githubusercontent.com/render/math?math=$\beta$"> coefficient can be calculated 
as the covariance of X and Y.

```{r, echo = T}
galtonNormalized = mutate(galton, child = (child - mean(child))/sd(child), parent = (parent - mean(parent))/sd(parent))
Beta = as.numeric(coef(lm(galtonNormalized$child ~ galtonNormalized$parent - 1)))
BetaCov = cor(galtonNormalized$child, galtonNormalized$parent)
angle = round(atan(Beta)*180/3.14,2)
mse = mean( (galtonNormalized$child - Beta * galtonNormalized$parent)^2 )
```  
Using the lm function we find the slope of line to be `r round(Beta, 3)`, and that the 
line that best fits the data originating from the slope is angled at `r angle` 
degrees. Using this value of Beta we can manually find the regression line that
fits the data and the mean square error is found to be `r round(mse, 3)`

```{r}
ggplot(
  galtonNormalized,
  aes(
    x = parent,
    y = child
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.1) +
  labs(x = "parent", y = "child") +
  geom_abline(slope = Beta, lwd = 4, col = "hotpink2") + 
  geom_point(aes(x = 0, y = 0), cex = 2, pch = 21) +
  geom_smooth(method = "lm", formula = y~x-1, se = F, col = "skyblue", lwd = 2, lty = 2) + 
  ggtitle(paste("betaCov = ", round(BetaCov,3),"beta = ", round(Beta,3), "mse = ", round(mse, 3))) +
  theme_bw() + 
  coord_cartesian(xlim = range(galtonNormalized$parent), ylim = range(galtonNormalized$child))
```   

We get the exact same result in this case as well.

### Performing regression analysis with the y-intercept
Complete regression line using the y-intercept

```{r, echo = T}
Beta = as.numeric(coef(lm(galton$child ~ galton$parent)))
Beta0 = Beta[1]
Beta1 = Beta[2]
angle = round(atan(Beta1)*180/3.14,2)
mse = mean( (galton$child - (Beta0 + Beta1 * galton$parent))^2 )
```  
Using the lm function we find the slope of line to be `r round(Beta, 3)`, and that the 
line that best fits the data originating from the slope is angled at `r angle` 
degrees. Using this value of Beta we can manually find the regression line that
fits the data and the mean square error is found to be `r round(mse, 3)`
```{r}
ggplot(
  galton,
  aes(
    x = parent,
    y = child
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.1) +
  labs(x = "parent", y = "child") +
  geom_abline(intercept = Beta0,slope = Beta1, lwd = 4, col = "hotpink2") + 
  geom_point(aes(x = 0, y = 0), cex = 2, pch = 21) +
  geom_smooth(method = "lm", formula = y~x, se = F, col = "skyblue", lwd = 2, lty = 2) + 
  ggtitle(paste("beta = ", round(Beta,3), "mse = ", round(mse, 3))) +
  theme_bw() + 
  coord_cartesian(xlim = range(galton$parent), ylim = range(galton$child))
```

We get the exact same result as seen in the normalized data regression above.


## Residuals

Consider the diamong dataset imported from the usingR package in R.
We can plot the scatterplot of x vs y and fit a regression line through them.

Here the residuals of a fitted line is a vector of numbers centered around zero
```{r}
y = diamond$price
x = diamond$carat
n = length(y)
fit = lm(y~x)
e = resid(fit) # gives vector of residuals which is part of the lm object referenced here as "fit"
yhat = predict(fit) # since no new data given yhat here will be the points on the fitted regression line
print(max(abs(e-(y-yhat))))
print(max(abs(e - (y - fit$coefficients[1] - fit$coefficients[2]*x))))
```  

```{r}
ggplot(
  diamond,
  aes(
    x = carat,
    y = price
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.6) +
  labs(x = "carat", y = "price") +
  geom_smooth(method = "lm", formula = y~x, se = F, col = "skyblue", lwd = 1, lty = 1) + 
  ggtitle("Carat vs Price") +
  theme_bw() + 
  coord_cartesian(xlim = range(diamond$carat), ylim = range(diamond$price)) +
  geom_segment(aes(x=x,y=y,xend=x,yend=yhat),col = "hotpink3")

```  

The above visualization isn't particularly useful for assessing the residual 
variation, a better alternative would be to plot the residuals on the verticle 
axis and independent variable on the horizontal axis.

```{r}
temp_data = data.frame(x = diamond$carat, y = e)
ggplot(
  temp_data,
  aes(
    x = x,
    y = y
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.6) +
  labs(x = "carat", y = "residuals") +
  ggtitle("Mass vs Residuals") +
  theme_bw() +
  geom_hline(aes(yintercept=0),col = "skyblue", lwd=1) +
  coord_cartesian(xlim = range(temp_data$x), ylim = range(temp_data$y)) +
  geom_segment(aes(x=x,y=0,xend=x,yend=y),col = "hotpink3")
```   

We look for any sort of patterns in the residual plot, ideally a residual plot 
should nothave any patterns.  

## Illustration of inference on regression
Illustration to show the inference on the regression line is identical to the 
output summary statistics generated by the the lm function.  
```{r}
library(UsingR); data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
e <- y - beta0 - beta1 * x
sigma <- sqrt(sum(e^2) / (n-2)) 
ssx <- sum((x - mean(x))^2)
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma 
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0; tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
print(coefTable)
fit <- lm(y ~ x); 
print(summary(fit)$coefficients)
```  

The confidence interval of the slope and the intercept of the regression line 
can be calculated as
```{r}
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10
```  
Which tells us that - "With a 95% confidence interval a 0.1 increase in the 
carat size we see a resultant increase in price in the range 356 to 389. 


## Multivariable regression 

### Example 1
Function to replace a variable in dataset with the residuals of the variable 
w.r.t the predictor.
```{r}
# Regress the given variable on the given predictor,
# suppressing the intercept, and return the residual.
regressOneOnOne <- function(predictor, other, dataframe){
  # Point A. Create a formula such as Girth ~ Height -1
  formula <- paste0(other, " ~ ", predictor, " - 1")
  # Use the formula in a regression and return the residual.
  resid(lm(formula, dataframe))
}

# Eliminate the specified predictor from the dataframe by
# regressing all other variables on that predictor
# and returning a data frame containing the residuals
# of those regressions.
eliminate <- function(predictor, dataframe){
  # Find the names of all columns except the predictor.
  others <- setdiff(names(dataframe), predictor)
  # Calculate the residuals of each when regressed against the given predictor
  temp <- sapply(others, function(other)regressOneOnOne(predictor, other, dataframe))
  # sapply returns a matrix of residuals; convert to a data frame and return.
  as.data.frame(temp)
}
```

We can use this function to reduce a multivariable regression in n variable to 
regression in one variable.

Using the trees dataset which can be used to fit a model to compute the volume 
of a tree given the girth and height of the tree.  

```{r}
head(trees)
```


A residual is the difference between a variable and its predicted value.
```{r}
fit <- lm(Volume ~ ., trees)
summary(fit)
```  

Adding constant column
```{r}
trees$Constant=1
```



Eliminating the "Girth" variable.
```{r}
trees2 <- eliminate("Girth", trees)
head(trees2)
```
This is regression in many variables amounts to a series of regressions in one.  

```{r}
fit2 <- lm(Volume ~ . - 1, trees2)
lapply(list(fit,fit2),coef)
```  

### Example 2
Using the mtcars dataset, fitting a linear model to estimate the mpg given cyl 
count, weight of vehicle wt, and performance in horsepower hp.
```{r}
data = select(mtcars,wt,cyl,hp,mpg)
rownames(data) <- NULL
data$const = 1
head(data)
```  
Fitting a linear model on the original dataset with all 4 parameters (including 
the constant parameter 1 which estimates the mean for the outcome)
```{r}
fit = lm(mpg~.-1,data)
fit
```


Removing the wt parameter by regressing all other parameters with the wt 
parameter to predict the values of other parameters which estimates the wt
```{r}
const = as.numeric(resid(lm(const~wt-1,data)))
hp = as.numeric(resid(lm(hp~wt-1,data)))
cyl = as.numeric(resid(lm(cyl~wt-1,data)))
mpg = as.numeric(resid(lm(mpg~wt-1,data)))
data2 = data.frame(const,hp,cyl,mpg)
head(data2)
```

Fitting a linear model on dataset with wt parameter removed
```{r}
fit2 = lm(mpg~.-1,data2)
```

Comparing the prediction of each models
```{r}
paste("Pred with wt: ",as.character(predict(fit, newdata = data.frame(wt=mean(data$wt),cyl=median(data$cyl),hp=mean(data$hp),const=median(data$hp)))))
paste("pred without wt: ",as.character(predict(fit2, newdata = data.frame(cyl=median(data$cyl),hp=mean(data$hp),const=median(data$hp)))))
```  
### Example 3
Now, considering a new subset of original dataset without the constant column,  
Regressing hp on cyl, wt, and also regressing on the response mpg
```{r}
# The intercept constant 1s are by default included
data = select(mtcars,wt,cyl,hp,mpg)
ex = as.numeric(resid(lm(hp~wt+cyl,data)))
ey = as.numeric(resid(lm(mpg~wt+cyl,data)))
```

ex represents the values of hp regressed on wt, cyl and column of 1s.  
ey represents the values of mpg regressed on wt, cyl and column of 1s.  
Since it is a linear model, the property of linearity holds - we can regress ey 
on ex(minus the column of 1s - since we already regressed both ex and ey on 1 
already) to get same response as we would get if we had regressed mpg on column 
of 1s, wt, cyl and hp.  
```{r}
fit1 = lm(ey~ex-1)
fit2 = lm(mpg~.,data)
x=round(as.numeric(summary(fit1)$resid),13);y=round(as.numeric(summary(fit2)$resid),13)
identical(x,y)
```   
The values of residuals are practically identical up to 13 decimal numeric 
precision.  

## Swiss dataset

Summarizing the variables in the dataset
```{r}
require(datasets); data(swiss); require(GGally); require(ggplot2)
smooth_plot_fn = function(data, mapping, method="loess",...){
  g = ggplot(
    data = data, 
    mapping = mapping
  ) + theme_bw() + 
    geom_point(col = "skyblue") + 
    geom_smooth(method = method, formula = y~x, ...) 
  g
}
ggpairs(swiss, lower = list(continuous=wrap(smooth_plot_fn,se=F,col="orange")), progress = F)
```   
### Regression on all the variables for illustration purpose
```{r}
summary(lm(Fertility~.,swiss))$coefficients
```  
This tells us thatL
* There is an expected 0.17 decrease in fertility for unit 
increase (1% increase in percentage of males) in agriculture, holding the 
remaining variables constant.  
* The Std. Error tells us how precise this observation is.  
* t values is the t statistic obtained by dividing the Estimate by the Std. 
Error terms, used for hypothesis testing.  

## Regression on factor variables
when dealing with factor variables R by default sets a level as the default 
value and it is interpreted by the intercept coefficient, whereas all the other 
levels are interpreted as a comparison of it's level to the default level.  
If we are to subtract the intercept coefficient from a given one we get the 
relation of this coefficient with the outcome minus the default.  

```{r, fig.height=5, fig.width=6}
require(datasets);data(InsectSprays); require(stats); require(ggplot2)
ggplot(
  data = InsectSprays,
  aes(
    y = count,
    x = spray,
    fill  = spray
  )
) + 
  geom_violin(colour = "black", size = 2) +
  xlab("Type of spray") + 
  ylab("Insect count")
```  

With spray group A as the reference the linear model coefficients are
```{r}
summary(lm(count ~ spray, data = InsectSprays))$coef
```  
This can be used to generate relevant inferences such as whether another spray 
is infact different from the reference spray using the t-statistic.  
And so we can test whether spray A is more effective than the others.  
To interpret the coeffients exactly as the means of the factors remove the 
x-intercept, as in 

```{r}
summary(lm(count ~ spray-1, data = InsectSprays))$coef
```  
The t-statistic in this case is telling us whether the mean is further away from 
zero i.e. whether a spray killed any insect and not with respect to how many a 
reference spray was able to kill.   

To change the reference level we use the relevel function.
```{r}
spray2 = relevel(InsectSprays$spray, "C")
summary(lm(count ~ spray2, data = InsectSprays))$coef
```   

### Illustration of regression with a continous and a factor variable
Continous variable - Agriculture  
Factor variable - CatholicBin (Whether >0.5 percent in a locality catholic or
not)  
```{r, fig.height=5, fig.width=7}
swiss = mutate(swiss, CatholicBin = 1 * (Catholic > 50))
g = ggplot(
  swiss, 
  aes(
    x = Agriculture,
    y = Fertility, 
    colour = factor(CatholicBin)
    )
  ) + geom_point(size = 6, colour = "black") +
  geom_point(size = 4) + 
  xlab("% in Agriculture") +
  ylab("Fertility")
g
```

**When disregarding the factor variable - Baseline**
```{r, fig.width=7}
fit = lm(Fertility ~ Agriculture, data = swiss)
g + geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], size = 2)
```  

Intercept = `r coef(fit)[1]`  
Slope = `r coef(fit)[2]`  

**Using factor - Scenario 1 - Only changing the intercept**  

Adding the factor variable creates just 3 coefficients
```{r}
fit = lm(Fertility ~ Agriculture + factor(CatholicBin), data = swiss)
summary(fit)$coef
```


```{r, fig.width=7}
g + geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], size = 2) +
  geom_abline(intercept = coef(fit)[1] + coef(fit)[3], slope = coef(fit)[2], size = 2, col = 2)
```  
Intercept = `r coef(fit)[1] + coef(fit)[3]`  
Slope = `r coef(fit)[2]`

**Using factor - Scenario 2 - Changing slope and intercept**  

When using 'variable * factor', R creates coefficients of variables in 
multiples of factor levels.  
In this case the factor has two levels, and by default the variable would have 2
coefficients the slope and the intercep, when using the *, there will be 4
coefficients.
```{r, fig.width=7}
fit = lm(Fertility ~ Agriculture * factor(CatholicBin), data = swiss)
summary(fit)$coef
```  
Since the coef[1], coef[2] are the reference, whereas the coef[3] is the 
deviation of intercept from reference when <img src="https://render.githubusercontent.com/render/math?math=$X_{2}=1$">
and coef[4] is deviation of slope coefficient from the reference similarly.

```{r}
g + geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], size = 2) +
  geom_abline(intercept = coef(fit)[1] + coef(fit)[3], slope = coef(fit)[2] + coef(fit)[4], size = 2, col = 2)
```

Intercept = `r coef(fit)[1] + coef(fit)[3]`  
Slope = `r coef(fit)[2] + coef(fit)[4]`


## Regression Multivariable - factors 
Fit a model with mpg as the outcome that includes number of cylinders as a 
factor variable and weight as a possible confounding variable. Compare the 
effect of 8 versus 4 cylinders on mpg for the adjusted and unadjusted by weight
models.  




Here, adjusted means including the weight variable as a term in the
regression model and unadjusted means the model without weight included.  
What can be said about the effect comparing 8 and 4 cylinders after looking at 
models with and without weight included?.  
```{r}
print(paste('Unadjusted coefficient:',as.character(summary(lm(mpg~cyl,mtcars))$coef[3])))
print(paste('Adjusted coefficient:',as.character(summary(lm(mpg~cyl+wt,mtcars))$coef[3])))
```

Holding weight constant, cylinder appears to have less of an impact on mpg than 
if weight is disregarded.  

Fit a model with mpg as the outcome that considers number of cylinders as a 
factor variable and weight as confounder. Now fit a second model with mpg as the 
outcome model that considers the interaction between number of cylinders
(as a factor variable) and weight. Give the P-value for the likelihood ratio
test comparing the two models and suggest a model using 0.05 as a type I error
rate significance benchmark.
```{r}
fitNoInt = lm(mpg~cyl+wt,mtcars)
fitInt = lm(mpg~cyl*wt,mtcars)
lrtest(fitNoInt,fitInt) 
```  

From the likelihood ratio test, P-value is larger than 0.05. So, according to
our criterion, we would fail to reject, which suggests that the interaction 
terms may not be necessary.

### hatvalues of most influencial point
Consider the following data set  
```
  x <- c(0.586, 0.166, -0.042, -0.614, 11.72)  
  y <- c(0.549, -0.026, -0.127, -0.751, 1.344)  
```
Give the hat diagonal for the most influential point  
```{r}
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit = lm(y~x)
hatvalues(fit)
```
Consider the following data set  
```
  x <- c(0.586, 0.166, -0.042, -0.614, 11.72)  
  y <- c(0.549, -0.026, -0.127, -0.751, 1.344)  
```
Give the slope dfbeta for the point with the highest hat value above.  
```{r}
dfbetas(fit)[5,2]
```

## Logistic regression
Consider the ravens win/loss rate dataset  
Analysing the win rate of Ravens by modelling on the Ravens score in various matches, fitting a binomial distribution to explain the data.
```{r}
if(!file.exists("./data/ravensData.rda")){
    download.file("https://raw.githubusercontent.com/jtleek/dataanalysis/master/week5/003countOutcomes/data/ravensData.rda",
                  destfile="./data/ravensData.rda",
                  method="curl")
}
load("./data/ravensData.rda")
head(ravensData)
```


Linear regression <img src="https://render.githubusercontent.com/render/math?math=$\mathrm{response}_{i}=b_{0}%2Bb_{1}\mathrm{predictor}_{i}%2Be_{i}$">   
Logistic regression <img src="https://render.githubusercontent.com/render/math?math=$\mathrm{log}\left(\frac{\mathrm{Pr}(response_{i}|predictor_{i},b_{0},b_{1})}{1%2D\mathrm{Pr}(response_{i}|predictor_{i},b_{0},b_{1})}\right)=b_{0}%2Bb_{1}predictor_{i}$">  
If, score==0 then we have  
<img src="https://render.githubusercontent.com/render/math?math=$\mathrm{log}\left(\frac{\mathrm{Pr}(response_{i}|predictor_{i},b_{0},b_{1})}{1%2D\mathrm{Pr}(response_{i}|predictor_{i},b_{0},b_{1})}\right)=b_{0}$">   
Thus,  
- <img src="https://render.githubusercontent.com/render/math?math=$b_{0}$" style="float:left"> - Log odds of Ravens win if they score zero points  
- <img src="https://render.githubusercontent.com/render/math?math=$b_{1}$" style="float:left"> - Log odds ratio of win probability for each point scored(relative to the zero point)  
- <img src="https://render.githubusercontent.com/render/math?math=$exp(b_{1})$" style="float:left"> - Odds ratio of win probability for each point scored(relative to the zero point)

and <img src="https://render.githubusercontent.com/render/math?math=$\frac{e^{b_{0}}}{1%2Be^{b_{0}}}$" style="display:inline"> is the probability of whether raven's win with score "0"  

To find the unit increase in probability of winning,  
- Let <img src="https://render.githubusercontent.com/render/math?math=$\b_{0}%2Bb_{1}(predictor_{i})$" style="display:inline"> be the probability of winning for given score, and     
- <img src="https://render.githubusercontent.com/render/math?math=$\b_{0}%2Bb_{1}(predictor_{i}%2B1)$" style="display:inline"> be the probability of winning for given score + 1, which means the probability given unit increase in score.    

Subtracting the terms we get, <img src="https://render.githubusercontent.com/render/math?math=$\b_{0}%2Bb_{1}(predictor_{i}%2B1)%2D\b_{0}%2Bb_{1}(predictor_{i})=b_{1}$">


