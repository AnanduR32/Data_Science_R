---
title: "regression"
author: "Anandu R"
date: "9/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height=5, fig.width=5, fig.align = "centred")
suppressMessages(
  {
    if(!require(manipulate)){
      install.packages("manipulate")
      library(manipulate)
    }
    library(UsingR)
    library(dplyr)
    data("galton")
    data("diamond")
  }
)
```

### Loading the galton data
```{r}
library(UsingR)
data("galton")
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData = mutate(freqData, child = as.numeric(as.vector(child)), parent = as.numeric(as.vector(parent)))
```

The data contains the heights of parents and child (paired). We try and estimate 
the child's height(y) using the height of the parents(x).

### Performing regression analysis without the y-intercept
With y-intercept 0 or in other words without a y-intercept, but this doesn't 
result in the best line to  fit the data, 
```{r, echo = T}
Beta = as.numeric(coef(lm(galton$child ~ galton$parent - 1)))
angle = round(atan(Beta)*180/3.14,2)
mse = mean( (galton$child - Beta * galton$parent)^2 )
```  
Using the lm function we find the slope of line to be `r round(Beta, 3)`, and that the 
line that best fits the data originating from the slope is angled at `r angle` 
degrees. Using this value of Beta we can manually find the regression line that
fits the data and the mean square error is found to be `r round(mse, 3)`
```{r}
ggplot(
  galton,
  aes(
    x = parent,
    y = child
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.1) +
  labs(x = "parent", y = "child") +
  geom_abline(slope = Beta, lwd = 4, col = "hotpink2") + 
  geom_point(aes(x = 0, y = 0), cex = 2, pch = 21) +
  geom_smooth(method = "lm", formula = y~x-1, se = F, col = "skyblue", lwd = 2, lty = 2) + 
  ggtitle(paste("beta = ", round(Beta,3), "mse = ", round(mse, 3))) +
  theme_bw() + 
  coord_cartesian(xlim = range(galton$parent), ylim = range(galton$child))
```

We see that the broken blue lines representing the regression line computed
using the lm() function overlaps with the pink line drawn using the Beta
coefficient estimated earlier.

### Regression with centered data
If we were to centre the data around the mean and perform the same analysis, 
we'd get the same result as we'd if the intercept estimator <img src="https://render.githubusercontent.com/render/math?math=$\beta_{0}$">  was under consideration 

```{r, echo = T}
galtonCentered = mutate(galton, child = child - mean(child), parent = parent - mean(parent))
Beta = as.numeric(coef(lm(galtonCentered$child ~ galtonCentered$parent - 1)))
angle = round(atan(Beta)*180/3.14,2)
mse = mean( (galtonCentered$child - Beta * galtonCentered$parent)^2 )
```  
Using the lm function we find the slope of line to be `r round(Beta, 3)`, and that the 
line that best fits the data originating from the slope is angled at `r angle` 
degrees. Using this value of Beta we can manually find the regression line that
fits the data and the mean square error is found to be `r round(mse, 3)`

```{r}
ggplot(
  galtonCentered,
  aes(
    x = parent,
    y = child
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.1) +
  labs(x = "parent", y = "child") +
  geom_abline(slope = Beta, lwd = 4, col = "hotpink2") + 
  geom_point(aes(x = 0, y = 0), cex = 2, pch = 21) +
  geom_smooth(method = "lm", formula = y~x-1, se = F, col = "skyblue", lwd = 2, lty = 2) + 
  ggtitle(paste("beta = ", round(Beta,3), "mse = ", round(mse, 3))) +
  theme_bw() + 
  coord_cartesian(xlim = range(galtonCentered$parent), ylim = range(galtonCentered$child))
```   

We see that the broken blue lines representing the regression line computed
using the lm() function overlaps with the pink line drawn using the Beta
coefficient estimated earlier. 

### Regression with normalized data
If we were to normalize the data and perform the same analysis, we'd get the
same result still, furthmore, the <img src="https://render.githubusercontent.com/render/math?math=$\beta$"> coefficient can be calculated 
as the covariance of X and Y.

```{r, echo = T}
galtonNormalized = mutate(galton, child = (child - mean(child))/sd(child), parent = (parent - mean(parent))/sd(parent))
Beta = as.numeric(coef(lm(galtonNormalized$child ~ galtonNormalized$parent - 1)))
BetaCov = cor(galtonNormalized$child, galtonNormalized$parent)
angle = round(atan(Beta)*180/3.14,2)
mse = mean( (galtonNormalized$child - Beta * galtonNormalized$parent)^2 )
```  
Using the lm function we find the slope of line to be `r round(Beta, 3)`, and that the 
line that best fits the data originating from the slope is angled at `r angle` 
degrees. Using this value of Beta we can manually find the regression line that
fits the data and the mean square error is found to be `r round(mse, 3)`

```{r}
ggplot(
  galtonNormalized,
  aes(
    x = parent,
    y = child
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.1) +
  labs(x = "parent", y = "child") +
  geom_abline(slope = Beta, lwd = 4, col = "hotpink2") + 
  geom_point(aes(x = 0, y = 0), cex = 2, pch = 21) +
  geom_smooth(method = "lm", formula = y~x-1, se = F, col = "skyblue", lwd = 2, lty = 2) + 
  ggtitle(paste("betaCov = ", round(BetaCov,3),"beta = ", round(Beta,3), "mse = ", round(mse, 3))) +
  theme_bw() + 
  coord_cartesian(xlim = range(galtonNormalized$parent), ylim = range(galtonNormalized$child))
```   

We get the exact same result in this case as well.

### Performing regression analysis with the y-intercept
Complete regression line using the y-intercept

```{r, echo = T}
Beta = as.numeric(coef(lm(galton$child ~ galton$parent)))
Beta0 = Beta[1]
Beta1 = Beta[2]
angle = round(atan(Beta1)*180/3.14,2)
mse = mean( (galton$child - (Beta0 + Beta1 * galton$parent))^2 )
```  
Using the lm function we find the slope of line to be `r round(Beta, 3)`, and that the 
line that best fits the data originating from the slope is angled at `r angle` 
degrees. Using this value of Beta we can manually find the regression line that
fits the data and the mean square error is found to be `r round(mse, 3)`
```{r}
ggplot(
  galton,
  aes(
    x = parent,
    y = child
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.1) +
  labs(x = "parent", y = "child") +
  geom_abline(intercept = Beta0,slope = Beta1, lwd = 4, col = "hotpink2") + 
  geom_point(aes(x = 0, y = 0), cex = 2, pch = 21) +
  geom_smooth(method = "lm", formula = y~x, se = F, col = "skyblue", lwd = 2, lty = 2) + 
  ggtitle(paste("beta = ", round(Beta,3), "mse = ", round(mse, 3))) +
  theme_bw() + 
  coord_cartesian(xlim = range(galton$parent), ylim = range(galton$child))
```

We get the exact same result as seen in the normalized data regression above.


## Residuals

Consider the diamong dataset imported from the usingR package in R.
We can plot the scatterplot of x vs y and fit a regression line through them.

Here the residuals of a fitted line is a vector of numbers centered around zero
```{r}
y = diamond$price
x = diamond$carat
n = length(y)
fit = lm(y~x)
e = resid(fit) # gives vector of residuals which is part of the lm object referenced here as "fit"
yhat = predict(fit) # since no new data given yhat here will be the points on the fitted regression line
print(max(abs(e-(y-yhat))))
print(max(abs(e - (y - fit$coefficients[1] - fit$coefficients[2]*x))))
```  

```{r}
ggplot(
  diamond,
  aes(
    x = carat,
    y = price
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.6) +
  labs(x = "carat", y = "price") +
  geom_smooth(method = "lm", formula = y~x, se = F, col = "skyblue", lwd = 1, lty = 1) + 
  ggtitle("Carat vs Price") +
  theme_bw() + 
  coord_cartesian(xlim = range(diamond$carat), ylim = range(diamond$price)) +
  geom_segment(aes(x=x,y=y,xend=x,yend=yhat),col = "hotpink3")

```  

The above visualization isn't particularly useful for assessing the residual 
variation, a better alternative would be to plot the residuals on the verticle 
axis and independent variable on the horizontal axis.

```{r}
temp_data = data.frame(x = diamond$carat, y = e)
ggplot(
  temp_data,
  aes(
    x = x,
    y = y
  )
) + geom_point(pch = 21, col = "blue", bg = "hotpink4",
               cex = 3, alpha = 0.6) +
  labs(x = "carat", y = "residuals") +
  ggtitle("Mass vs Residuals") +
  theme_bw() +
  geom_hline(aes(yintercept=0),col = "skyblue", lwd=1) +
  coord_cartesian(xlim = range(temp_data$x), ylim = range(temp_data$y)) +
  geom_segment(aes(x=x,y=0,xend=x,yend=y),col = "hotpink3")
```   

We look for any sort of patterns in the residual plot, ideally a residual plot 
should nothave any patterns.  

## Illustration of inference on regression
Illustration to show the inference on the regression line is identical to the 
output summary statistics generated by the the lm function.  
```{r}
library(UsingR); data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
e <- y - beta0 - beta1 * x
sigma <- sqrt(sum(e^2) / (n-2)) 
ssx <- sum((x - mean(x))^2)
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma 
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0; tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
print(coefTable)
fit <- lm(y ~ x); 
print(summary(fit)$coefficients)
```  

The confidence interval of the slope and the intercept of the regression line 
can be calculated as
```{r}
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10
```  
Which tells us that - "With a 95% confidence interval a 0.1 increase in the 
carat size we see a resultant increase in price in the range 356 to 389. 







