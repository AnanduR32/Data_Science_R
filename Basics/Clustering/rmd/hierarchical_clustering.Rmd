---
title: "ggplot"
author: "Anandu R"
date: "6/15/2020"
output:
  html_document:
    df_print: paged
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# hierarchical Clustering 

Hierarchical clustering is an agglomerative, or bottom-up, approach. -    "each 
observation starts in its own cluster, and pairs of clusters are merged as one
moves up the hierarchy." This means that we'll find the closest two points and
put them together in one cluster, then find the next closest pair in the updated
picture, and so forth. We'll repeat this process until we reach a reasonable 
stopping place. Wikipedia tells us, "one can decide to stop clustering either
when the clusters are too far apart to be merged (distance criterion) or when
there is a sufficiently small number of clusters (number criterion)."


Illustrating clusters 

```{r}
set.seed(96)
x = rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y = rnorm(12, mean = rep(c(1,2,1),each = 4), sd = 0.2)
plot(x, y, col = "orange", pch = 19,cex = 1.5)
text(x+0.05, y+0.05, labels = as.character(1:12))
```

### Step 1

One method to measure distance would be to use euclidean distance, which uses a 
form of pythagorous theorm to measure the diagonal(direct) distance between two
or more points. Another method would be to use the City Block distance measure 
or Manhatten distance.
The Manhattan distance is the sum of the absolute values of the distances 
between each coordinate, so the distance between the points
| (x1,y1) and (x2,y2) is |x1-x2|+|y1-y2|. Both these can be generalized to 
finitely many dimsensions.

Find the pairwise distance between each points using the dist() function, which
uses Eucledian distance as the measure. 

```{r}
data = data.frame(x,y)
distxy = dist(data)
distxy
```

### Step 2

The clusters can be defined using two modes of distance measure, one is complete 
linkage - all points(clusters) are grouped into pairs and distance is found out, 
the points that are most distant are to be categorized into different clusters, 
closest ones(from individual clusters) are made into a single cluster in the 
dendrogram.

Another way to measure distance between cluster is average linkage, were in we 
compute average point in each cluster and and measure the distance between each 
cluster.

The number of clusters you derive from your data depends on the distance at
which you choose to cut it.

Hierarchical Clustering using hclust()

```{r}
hClustering = hclust(distxy)
plot(hClustering)
```

```{r plclust}
myplclust <- function(
  hclust, lab=hclust$labels, lab.col=rep(
    1,length(hclust$labels)
    ), hang=0.1,...
  ){
  ## modifiction of plclust for plotting hclust objects *in colour*!
  ## Copyright Eva KF Chan 2009
  ## Arguments:
  ##    hclust:    hclust object
  ##    lab:        a character vector of labels of the leaves of the tree
  ##    lab.col:    colour for the labels; NA=default device foreground colour
  ##    hang:     as in hclust & plclust
  ## Side effect:
  ##    A display of hierarchical cluster with coloured leaf labels.
  y <- rep(hclust$height,2); x <- as.numeric(hclust$merge)
  y <- y[which(x<0)]; x <- x[which(x<0)]; x <- abs(x)
  y <- y[order(x)]; x <- x[order(x)]
  plot( hclust, labels=FALSE, hang=hang, ... )
  text( x=x, y=y[hclust$order]-(max(hclust$height)*hang),
        labels=lab[hclust$order], col=lab.col[hclust$order], 
        srt=90, adj=c(1,0.5), xpd=NA, ... )
}
```

Using a custom function to plot the clusters 

```{r}
myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(2:4, each = 4))
```

## Using the heatmap function to derive correlation between points in data 

A heatmap is a graphical representation of data where the individual values 
contained in a matrix are represented as colors.

Inorder to be able to organise the data with some logical way 
```{r}
dataAsMatrix = as.matrix(data)[sample(1:12),]
heatmap(dataAsMatrix)
```

