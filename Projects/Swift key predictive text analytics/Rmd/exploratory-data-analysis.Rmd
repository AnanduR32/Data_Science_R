---
title: "Text mining"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
fontsize: 10pt
output:
  rmdformats::readthedown:
    code_download: yes

    df_print: paged
    theme: cosmo
    highlight: tango
    thumbnails: false
    lightbox: false
  html_document:
    code_download: yes
    df_print: paged
    fig_width: 8
    fig_height: 5
    theme: cosmo
    highlight: tango
    toc: yes
    toc_depth: 4
  word_document:
    toc: yes
    toc_depth: '4'
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: '3'
    keep_tex: true
header-includes:
- \DeclareMathOperator*{\argmin}{arg\,min}
- \newcommand*{\prob}{\mathsf{P}}

urlcolor: 'blue'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center", fig.width = 8, fig.height = 6
)
```

# Introduction
## Understanding the problem 
<p align="justify">
NLP is a hot topic and is being used widely in the industry to penetrate deeper
into user analytics and understand the user better, learn the vocabulory and 
suggest auto-completion, analyze behaviour and so on.</p>

<p align="justify">
Here use apply the knowledge to build a text/sentence auto completion product 
which requires us to: </p>
<ol>
  <li>Analyzing a large corpus of text documents to discover the structure in the data 
and how words are put together. </li>
  <li>Cleaning and analyzing text data, then building and sampling from a predictive 
text model</li>
  <li>Finally, to build a predictive text product.</li>
</ol>

## Packages used for analysis
```{r message=FALSE, warning=FALSE}
require(ngram)
require(NLP)
require(tm)
require(RWeka)
require(data.table)
require(corpus)
require(qdap)
require(ggplot2)
require(tidytext)
require(stringi)
require(stringr)
require(dplyr)
require(viridisLite)
require(plotrix)
require(dendextend)
```


# Data gathering and info

<p align="justify">
Data exploration and understanding is a key aspect of data science, for the 
purpose of understanding the kind of data we are working with.</p>

## Getting the data

<p align="justify">
The data is obtained as part of the coursera datascience specialization capstone 
project, and consists of 4 folders that each consist of data from twitter, news 
and blogs. Each folder represents languages: English, Finnish, German and 
Russian.</p>

```{r message=FALSE, warning=FALSE}
fileUrl = 
  'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
dir.create(file.path('../data'), recursive = TRUE,showWarnings = F)
if(!dir.exists('../data/en_US')){
  download.file(fileUrl,'../data/dataset.zip', mode = 'wb')
  unzip("../data/dataset.zip", exdir = '../data')
  file.copy("../data/final/en_US", "../data", recursive = T)
  file.copy("../data/final/de_DE", "../data", recursive = T)
  file.copy("../data/final/fi_FI", "../data", recursive = T)
  file.copy("../data/final/ru_RU", "../data", recursive = T)
}
rm(fileUrl)
unlink('../data/final', recursive = T)
unlink("../data/dataset.zip")
```

## Loading the en_US blogs, news and twitter data
<p align="justify">
The data extracted consists of just lines of text/sentences that are to be mined 
to distill actionable insights.
The files are read one by one using the readLines() function with encoding set 
to utf-8, the standard encoding standard, skipping through the null lines.</p>

```{r message=FALSE, warning=FALSE}
en_blogs = readLines(
  "../data/en_US/en_US.blogs.txt", encoding="UTF-8", 
  skipNul = TRUE, warn = TRUE)
en_news = readLines(
  "../data/en_US/en_US.news.txt", encoding="UTF-8", 
  skipNul = TRUE, warn = TRUE)
en_twitter = readLines(
  "../data/en_US/en_US.twitter.txt", encoding="UTF-8", 
  skipNul = TRUE, warn = TRUE)
```

## Summary statistics on the data
```{r}
blogs_info = c(stri_stats_general(en_blogs)[1],  stri_stats_latex(en_blogs)[4],
               max(summary(nchar(en_blogs))),
               sum(grepl("love", en_blogs))/sum(grepl("hate", en_blogs)),
               file.info("../data/en_US/en_US.blogs.txt")$size/(2^20))
news_info = c(stri_stats_general(en_news)[1], stri_stats_latex(en_news)[4],
              max(summary(nchar(en_news))), 
              sum(grepl("love", en_news))/sum(grepl("hate", en_news)),
              file.info("../data/en_US/en_US.news.txt")$size/(2^20))
twitter_info = c(stri_stats_general(en_twitter)[1], stri_stats_latex(en_twitter)[4],
                 max(summary(nchar(en_twitter))), 
                 sum(grepl("love", en_twitter))/sum(grepl("hate", en_twitter)),
                 file.info("../data/en_US/en_US.twitter.txt")$size/(2^20))
total_info = blogs_info+news_info+twitter_info

table_info <- as.data.frame(rbind(blogs_info, news_info, twitter_info, total_info))
rm(blogs_info, news_info, twitter_info, total_info)
colnames(table_info) = c("Lines", "Words", "Longest line", 'love/hate ratio', "Size in Mb")
table_info
```
# data processing
## Sampling and combining the data
```{r sampling-data}
set.seed(193)
sample_size = 2500
blogs = en_blogs[sample(1:length(en_blogs),sample_size)]
blogs = removeWords(blogs, stopwords("en"))
blogs = gsub("\\s+", " ", blogs)
blogs = str_trim(blogs, side = c("both"))
news = en_news[sample(1:length(en_news),sample_size)]
news = removeWords(news, stopwords("en"))
news = gsub("\\s+", " ", news)
news = str_trim(news, side = c("both"))
twitter = en_twitter[sample(1:length(en_twitter),sample_size)]
twitter = removeWords(twitter, stopwords("en"))
twitter = gsub("\\s+", " ", twitter)
twitter = str_trim(twitter, side = c("both"))
rm(sample_size, en_blogs, en_news, en_twitter)
```

## Creating volatile corpus document
### Building a corpus document
<p align="justify">A corpus is a collection of documents, We create a volatile corpus to from the 
vector of texts obtained.</p>

<p align="justify">Make a vector source using the tm package, then converting 
the source vector into a VCorpus object</p>
```{r build-corpus}
create_corpus = function(text_data){
  text_corpus = VCorpus(
    VectorSource(text_data),
      readerControl=list(readPlain, language="en", load=TRUE)
  )
  text_corpus
}
blogs_corpus = create_corpus(blogs)
news_corpus = create_corpus(news)
twitter_corpus = create_corpus(twitter)
```

<p align="justify">The VCorpus object uses a nested - list of list structure to 
hold the data. At each index of the VCorpus object, there is a PlainTextDocument
object, which is a list containing actual text data (content), and some
corresponding metadata (meta). It can help to visualize a VCorpus object to 
conceptualize the whole thing.</p>

### Cleaning and preprocessing text
<p align="justify">Using tm's built-in text processing methods to mine data from 
the corpus.</p>
```{r clean-corpus}
clean_corpus <- function (corpus) {
    corpus <- tm_map(corpus, tolower) # all lowercase
    corpus <- tm_map(corpus, removePunctuation) # Eleminate punctuation
    corpus <- tm_map(corpus, removeNumbers) # Eliminate numbers
    corpus <- tm_map(corpus, replace_abbreviation) # Eliminate abbreviations
    corpus <- tm_map(corpus, replace_contraction) # Eliminate contractions
    corpus <- tm_map(corpus, replace_symbol) # Eliminate symbols
    corpus <- tm_map(corpus, stripWhitespace) # Strip Whitespace
    corpus <- tm_map(corpus, removeWords, stopwords("english")) # Eliminate English stop words
    # corpus <- tm_map(corpus, stemDocument) # Stem the document
    corpus <- tm_map(corpus, PlainTextDocument) # Create plain text format
}

text_corpus = c(news_corpus,blogs_corpus,twitter_corpus, recursive = FALSE)
text_corpus_cleaned = clean_corpus(text_corpus)

# Comparing with original data
cat("Original document: ",content(text_corpus[[1]]))
cat("\nCleaned document: ",content(text_corpus_cleaned[[1]]))

## Saving corpus cleaned 
getwd()
writeLines(as.character(text_corpus_cleaned), con="../Corpus/text_corpus_cleaned.txt")

# Disposing off the original corpus
rm(text_corpus, blogs_corpus, news_corpus, twitter_corpus)
```

## Creating a document-term matrix for analysis
<p align="justify">When we wish to represent the data with the document as rows and the words as 
column we use document term matrix, the transpose of which is term document 
matrix.<br>
The fields then represent the frequency of words in the data. However, other
frequency measures do exist.</p>

```{r create-TermDocument-Matrix}
text_tdm = TermDocumentMatrix(text_corpus_cleaned)
# rm(text_corpus_cleaned)
text_tdm
```

## Convert term document matrix to matrix
The term document is converted into a large matrix in which each cell represents 
the count/frequency of the word(row) in the corresponding document(column). 
```{r convert-tdm-to-m}
text_m = as.matrix(text_tdm)
```

# Exploratory analysis
<p align="justify">Using qdap's fre_terms() functions to count the frequency of 
words in the datasets and presenting the information in tabular and graphical 
methods.</p>

## Most frequently used words
```{r term-frequency-barplot}
term_frequency = rowSums(text_m)
term_frequency <- sort(term_frequency, decreasing = T)
rm(text_m)
barplot(term_frequency[1:10], col = "hotpink4", las = 2, main = "Most frequently used words")
```

## Sentiment analysis 
### Creating a tibble 
<p align="justify">Converting the twitter_dtm to a tibble for sentiment analysis using tidytext
package.</p>
```{r sentiment-analysis-init}
text_tbl = tidytext::tidy(text_tdm)
text_tbl = text_tbl[,c("term","count")]
print(dim(text_tbl))
head(text_tbl[order(-text_tbl$count),], 10)
```

### Forming analysis
```{r sentiment-analysis-create}
text_sentiments <- text_tbl %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))
print(dim(text_sentiments))
head(text_sentiments, 10)
```

### Visualizing
```{r sentiment-analysis-plot}
text_sentiments %>%
  count(sentiment, term, wt = count) %>%
  filter(n >= 35) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) + 
  theme_minimal() +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle=90,hjust=1)) + #element_blank()) +
  ylab("Contribution to sentiment")
```

```{r clean-up-trigrams, echo = F, error=F, warning=F}
rm(text_tbl, text_sentiments)
```

## Word cloud 
```{r word-cloud-text}
word_freqs = data.frame(
  term = names(term_frequency),
  num = term_frequency
)
wordcloud::wordcloud(
  word_freqs$term, word_freqs$num, 
  max.words = 100, colors = cividis(n = 3)
)
```
```{r clean-up-cloud-freq, echo = F, error=F, warning=F}
rm(word_freqs)
```

### Comparisons using word cloud 

<p align="justify">Preparing the data</p>
```{r preprocess-for-wordcloud-comparisons}
all_blogs = paste(blogs, collapse = "")
all_news = paste(news, collapse = "")
all_twitter = paste(twitter ,collapse = "")
all_texts = c(all_blogs, all_news, all_twitter)

rm(blogs, news, twitter)
all_texts = VCorpus(VectorSource(all_texts))
# all_texts = clean_corpus(all_texts)

all_dm = TermDocumentMatrix(all_texts)
all_tdm = TermDocumentMatrix(all_texts)
colnames(all_tdm) = c("Blogs", "News", "Twitter")
all_m = as.matrix(all_dm)
all_mc = as.matrix(all_tdm)
```

#### Commonality
<p align="justify">Using commonality word cloud to how similar the three corpus 
are</p>
```{r wordcloud-intersection, warning=F}
wordcloud::commonality.cloud(all_m, colors = magma(n = 5), max.words = 100)
```  

```{r}
rm(all_m, all_dm)
```


#### Comparison
<p align="justify">Using comparison cloud to understand the distribution of 
words in each category of texts - blogs, news and twitter</p>
```{r word-cloud-difference, warning=F}
wordcloud::comparison.cloud(all_mc, colors = c("orange","skyblue","hotpink3"), max.words = 100)
```

## Polaized tag plot 

### Creating data for plot
<p align="justify">using the pyramid package </br>
blogs and news: </p>
```{r}
# subset the common words in the documents
common_words = as.data.frame(subset(all_mc, all_mc[,1]>0 & all_mc[,2]>0 & all_mc[,3]>0))
# Finding the difference and then ordering by it
common_words = mutate(common_words, 
                      diff_bn = abs(common_words[,1] - common_words[,2]),
                      diff_bt = abs(common_words[,1] - common_words[,3]),
                      diff_nt = abs(common_words[,2] - common_words[,3]),
                      labels = rownames(common_words)
)
common_words = common_words[order(-common_words[,4]),]
top_25_bn = common_words[1:25,]
common_words = common_words[order(-common_words[,5]),]
top_25_bt = common_words[1:25,]
common_words = common_words[order(-common_words[,6]),]
top_25_nt = common_words[1:25,]
```

### Difference between common terms in blogs and news
```{r pyramid-plot-blogs-vs-news}
pyramid.plot(
  top_25_bn$Blogs, top_25_bn$News, labels = top_25_bn$labels,
  main = "Words in common between blogs and news", gap = 150,
  unit = NULL, raxlab = NULL, laxlab = NULL, top.labels = 
    c("blogs", "words", "news")
)
```


### Difference between common terms in blogs and twitter
```{r pyramid-plot-blogs-vs-twitter}
pyramid.plot(
  top_25_bt$Blogs, top_25_bt$News, labels = top_25_bt$labels,
  main = "Words in common between blogs and news", gap = 100,
  unit = NULL, raxlab = NULL, laxlab = NULL, top.labels = 
    c("blogs", "words", "twitter")
)
```


### Difference between common terms in news and twitter
```{r pyramid-plot-news-vs-twitter}
pyramid.plot(
  top_25_nt$Blogs, top_25_nt$News, labels = top_25_nt$labels,
  main = "Words in common between blogs and news", gap = 200,
  unit = NULL, raxlab = NULL, laxlab = NULL, top.labels = 
    c("news", "words", "twitter")
)
``` 

```{r clean-up-all, echo = F, error=F, warning=F}
rm(top_25_nt, common_words, top_25_bt, top_25_bn, all_mc, all_texts, all_twitter, all_news, all_blogs, all_tdm)
```


## Analyzing words using dendrogram plot
<p align="justify">Dendrograms reduce complicated multi-dimensional datasets to simple clustering
information. This makes them a valuable tool to reduce complexity. Using the
distance matrix and hclust() function we create a hierarchical cluster object
which is then passed into the function as.dendrogram.</br>
</br>
Removing sparsity of the term document using the removeSparseTerms to reduce the
number of terms, the parameter 'sparse' species the percent cut-off for number 
of zeroes that is allowed for each term.</p>
```{r preparedata-dendrogram}
# Removing sparse terms 
text_reduced = removeSparseTerms(text_tdm, sparse = 0.97)
text_reduced_m = as.matrix(text_reduced)
text_reduced_dist = dist(text_reduced_m)

# Creating hclust object
text_hc = hclust(text_reduced_dist)

# Converting hclust object to dendrogram
text_dend = as.dendrogram(text_hc)

# Labels of dendrogram object
labels(text_dend)
```
```{r dendrogram-plot}
# Changing color of branches 'year', 'love' and 'said' to red
text_dend_colored = branches_attr_by_labels(
  text_dend, 
  c("two","will","said"),
  "red"
)
# Plot
plot(text_dend_colored, main = "Dendrogram analysis")

# Adding rectangles
rect.dendrogram(tree = text_dend_colored, k = 3, border = "grey50")
```

```{r clean-up-dend, echo = F, error=F, warning=F}
rm(text_dend, text_dend_colored, text_hc, text_reduced, text_reduced_m)
```

## Word associations
<p align="justify">Using the findAssocs() function of tm package and calculating the correlation of
a word with every other words/terms in the document in the range [0,1].</p>
```{r association-analysis}
text_associations = findAssocs(text_tdm, "year", 0.125)

# Converting to dataframe for plot
text_associations_df = list_vect2df(
  text_associations, col2 = "word", col3 = "score"
)
# plot
ggplot(text_associations_df, aes(score, word)) +
  geom_point(size = 3) + 
  theme_minimal()
rm(text_associations, text_associations_df, text_tdm)
```

## N-gram tokenize
So far we've studied the association of a token containing just one word with 
other words/tokens, this part of the analysis concentrates on analysis of 
tokens containing more than one word.

### Defining functions
```{r tokenize-function-definition}
# Creating a tokenizer functions using the RWeka package
bigram_tokenizer = function(x){
  NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
trigram_tokenizer = function(x){
  NGramTokenizer(x, Weka_control(min = 3, max = 3))
}
tetragram_tokenizer = function(x){
  NGramTokenizer(x, Weka_control(min = 4, max = 4))
}
```

### Bigrams
#### Creating a bigram tdm
```{r}
bigram_tdm = TermDocumentMatrix(
  text_corpus_cleaned,
  control = list(tokenize = bigram_tokenizer)
)
bigram_m = as.matrix(bigram_tdm)
```
#### wordcloud analysis
```{r bigram-cloud, warning=F}
freq = rowSums(bigram_m)
bi_tokens = names(freq)
bi_token_df = data.frame(token = bi_tokens, freq = freq)
row.names(bi_token_df) = NULL
wordcloud::wordcloud(bi_token_df$token, bi_token_df$freq, max.words = 150, colors = cividis(n = 3))
```  

#### Subset top 1%
```{r}
## Identifying the 95th quantile
top_bigrams = bi_token_df %>%
  filter(freq > 1)
save(top_bigrams, file = "../NGrams/top_bigrams.rda")
```


```{r clean-up-bigrams, echo = F, error=F, warning=F}
rm(bigram_m, bigram_tdm, bi_tokens, freq, top_bigrams, bi_token_df)
```


### Trigrams
#### Creating a trigram tdm
```{r}
trigram_tdm = TermDocumentMatrix(
  text_corpus_cleaned,
  control = list(tokenize = trigram_tokenizer)
)
trigram_m = as.matrix(trigram_tdm)
```
#### wordcloud analysis
```{r trigram-cloud, warning=F}
freq = rowSums(trigram_m)
tri_tokens = names(freq)
tri_token_df = data.frame(token = tri_tokens, freq = freq)
row.names(tri_token_df) = NULL
wordcloud::wordcloud(
  words = tri_token_df$token, freq = tri_token_df$freq, 
  max.words = 150, colors = cividis(n = 3)
)
```  

#### Subset top 1%
```{r}
## Identifying the 95th quantile
top_trigrams = tri_token_df %>%
  filter(freq > 1)
save(top_trigrams, file = "../NGrams/top_trigrams.rda")
```

```{r clean-up-trigram, echo = F, error=F, warning=F}
rm(trigram_m, trigram_tdm, tri_tokens, freq, tri_token_df, top_trigrams)
```

### Tetragrams
#### Creating a tetragram tdm
```{r}
tetragram_tdm = TermDocumentMatrix(
  text_corpus_cleaned,
  control = list(tokenize = tetragram_tokenizer)
)
tetragram_m = as.matrix(tetragram_tdm)
```
#### wordcloud analysis
```{r tetragram-head, warning=F}
freq = rowSums(tetragram_m)
tetra_tokens = names(freq)
tetra_token_df = data.frame(token = tetra_tokens, freq = freq)
row.names(tetra_token_df) = NULL
head(tetra_token_df[order(-tetra_token_df$freq),])
```  
```{r tetragram-cloud, warning=F}
wordcloud::wordcloud(
  words = tetra_token_df$token, freq = tetra_token_df$freq, 
  max.words = 150, colors = cividis(n = 5)
)
```


#### Subset top 1%
```{r}
## Identifying the 95th quantile
top_tetragrams = tetra_token_df %>%
  filter(freq > 1)
save(top_tetragrams, file = "../NGrams/top_tetragrams.rda")
```


```{r clean-up-tetragrams, echo = F, error=F, warning=F}
rm(tetragram_m, tetragram_tdm, tetra_tokens, freq, top_tetragrams, tetra_token_df)
```

# Creating prediction model   
Creating a function that predicts the next word for a given input list of words  

```{r}
## Loading the n-grams 
top_bigrams = get(load("../NGrams/top_bigrams.rda"))
top_trigrams = get(load("../NGrams/top_trigrams.rda"))
top_tetragrams = get(load("../NGrams/top_tetragrams.rda"))


## Function
predictWord = function(str){
  preds_ = ""
  pred = ""
  str = removeWords(str, stopwords("en"))
  str = gsub("\\s+", " ", str)
  str = str_trim(str, side = c("both"))
  n_words = str_count(str, " ") + 1
  if(n_words == 3){
    matching_tetragrams = top_tetragrams[grepl(str, top_tetragrams[,1], ignore.case=TRUE),]
    preds_ = data.frame(prediction = str_extract_all(matching_tetragrams[,1], paste0(str,"\\s([:alpha:]+)"), simplify = T),freq = matching_tetragrams[,2])  
  }
  if(is.null(dim(preds_))){
    matching_trigrams = top_trigrams[grep(str, top_trigrams[,1], ignore.case=TRUE),]
    preds_ = data.frame(prediction = str_extract_all(matching_trigrams[,1], paste0(str,"\\s([:alpha:]+)"), simplify = T),freq = matching_trigrams[,2])  
  }
  if(is.null(dim(preds_))){
    matching_bigrams = top_bigrams[grep(paste("(^[:alpha:])",str), top_bigrams[,1], ignore.case=TRUE),]
    preds_ = data.frame(prediction = str_extract_all(matching_bigrams[,1], paste0(str,"\\s([:alpha:]+)"), simplify = T),freq = matching_bigrams[,2])  
  }
  pred = str_split_fixed(preds_[order(preds_[,2]),][1,1], " ", n=2)[2]
  pred
}
```



\footnotesize

```{r session-end, echo = FALSE, results='hold'}
options(width = 100)

## Deleting data after analysis
unlink("../data",recursive = T)

cat("R Session Info:\n")
sessionInfo()
```
